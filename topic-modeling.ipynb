{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bertopic datasets -q\n",
    "#!pip install umap-learn altair annoy   -q\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "import altair as alt\n",
    "\n",
    "def load_and_preprocess_data(dataset_name=\"Intel/orca_dpo_pairs\"):\n",
    "    \"\"\"\n",
    "    Load the dataset and preprocess it by concatenating questions and chosen answers.\n",
    "    Returns a DataFrame with processed text and original questions for titles.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs_ = load_dataset(dataset_name)[\"train\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "    docs_q_ = docs_[\"question\"]\n",
    "    docs_c_ = docs_[\"chosen\"]\n",
    "    docs = [x + \" \" + y for x, y in zip(docs_q_, docs_c_)]\n",
    "    \n",
    "    # Trim words to manage token sequence length effectively. \n",
    "    # A modified approach is required if a significant percentage of the content is consistenly longer \n",
    "    docs_q_trimmed = [x[:4000] for x in docs_q_]\n",
    "    \n",
    "    return pd.DataFrame({'title': docs_q_trimmed, 'text': docs})\n",
    "\n",
    "def compute_embeddings(docs):\n",
    "    \"\"\"\n",
    "    Compute embeddings for the given documents using a predefined model.\n",
    "    \"\"\"\n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
    "    embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def create_umap_embeddings(embeddings, n_neighbors=15):\n",
    "    \"\"\"\n",
    "    Reduce the dimensionality of embeddings using UMAP.\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=42)\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "    return umap_embeddings\n",
    "\n",
    "def cluster_data(embeddings, n_clusters=30):\n",
    "    \"\"\"\n",
    "    Cluster the embeddings into a specified number of clusters using KMeans.\n",
    "    \"\"\"\n",
    "    kmeans_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans_model.fit_predict(embeddings)\n",
    "    return clusters\n",
    "\n",
    "def extract_keywords(df):\n",
    "    \"\"\"\n",
    "    Extract and assign keywords for each cluster using TF-IDF.\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate scores for each word in each cluster\n",
    "    tfidf_scores = tfidf.toarray()\n",
    "    \n",
    "    # Extract the top 10 words per cluster\n",
    "    top_n_words = 10\n",
    "    keywords_per_cluster = {i: \", \".join(words[tfidf_scores[i].argsort()[-top_n_words:]]) for i in range(len(tfidf_scores))}\n",
    "    df['keywords'] = df['cluster'].apply(lambda x: keywords_per_cluster[x])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def visualize_clusters(df):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization of the clustered documents.\n",
    "    \"\"\"\n",
    "    chart = alt.Chart(df).mark_circle(size=60, stroke='#666', strokeWidth=1, opacity=0.3).encode(\n",
    "        x='x:Q',\n",
    "        y='y:Q',\n",
    "        color='keywords:N',\n",
    "        tooltip=['title', 'keywords', 'cluster']\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=500,\n",
    "        title='Intel Orca DPO Dataset Clusters'\n",
    "    ).interactive()\n",
    "    \n",
    "    return chart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_and_preprocess_data()\n",
    "    if df is not None:\n",
    "        embeddings = compute_embeddings(df['text'])\n",
    "        umap_embeddings = create_umap_embeddings(embeddings)\n",
    "        df['x'], df['y'] = umap_embeddings[:, 0], umap_embeddings[:, 1]\n",
    "        clusters = cluster_data(embeddings)\n",
    "        df['cluster'] = clusters\n",
    "        df = extract_keywords(df)\n",
    "        chart = visualize_clusters(df)\n",
    "        chart.display()\n",
    "    else:\n",
    "        print(\"Failed to load or preprocess data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
